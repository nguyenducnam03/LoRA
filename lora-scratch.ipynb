{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- Source: https://www.youtube.com/watch?v=PXWYUTMt-AU","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision.datasets as datasets \nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T02:34:29.223042Z","iopub.execute_input":"2024-12-31T02:34:29.223246Z","iopub.status.idle":"2024-12-31T02:34:29.227652Z","shell.execute_reply.started":"2024-12-31T02:34:29.223228Z","shell.execute_reply":"2024-12-31T02:34:29.226524Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Make torch deterministic\n_ = torch.manual_seed(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T02:34:29.230293Z","iopub.execute_input":"2024-12-31T02:34:29.230523Z","iopub.status.idle":"2024-12-31T02:34:29.249228Z","shell.execute_reply.started":"2024-12-31T02:34:29.230504Z","shell.execute_reply":"2024-12-31T02:34:29.248604Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"- We will be training a network to classify MNIST digits and then fine-tune the network on a particular digit on which it doesn't perform well.","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n\n# Load the MNIST dataset\nmnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n# Create a dataloader for the training\ntrain_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n\n# Load the MNIST test set\nmnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n\n# Define the device\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T02:34:33.893471Z","iopub.execute_input":"2024-12-31T02:34:33.893796Z","iopub.status.idle":"2024-12-31T02:34:38.972532Z","shell.execute_reply.started":"2024-12-31T02:34:33.893771Z","shell.execute_reply":"2024-12-31T02:34:38.971569Z"}},"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 16156831.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 478468.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1648877/1648877 [00:00<00:00, 4432235.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 403: Forbidden\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 5422866.15it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T02:34:41.050233Z","iopub.execute_input":"2024-12-31T02:34:41.050608Z","iopub.status.idle":"2024-12-31T02:34:41.056240Z","shell.execute_reply.started":"2024-12-31T02:34:41.050570Z","shell.execute_reply":"2024-12-31T02:34:41.055413Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(<torch.utils.data.dataloader.DataLoader at 0x7e6e81ea2980>,\n <torch.utils.data.dataloader.DataLoader at 0x7e6e81ea33a0>)"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"- Create the Neural Network to classify the digits, making it overly complicated to better show the power of LoRA","metadata":{}},{"cell_type":"code","source":"# Create an overly expensive neural network to classify MNIST digits\n# Daddy got money, so I don't care about efficiency\nclass RichBoyNet(nn.Module):\n    def __init__(self, hidden_size_1=1000, hidden_size_2=2000):\n        super(RichBoyNet,self).__init__()\n        self.linear1 = nn.Linear(28*28, hidden_size_1) \n        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2) \n        self.linear3 = nn.Linear(hidden_size_2, 10)\n        self.relu = nn.ReLU()\n\n    def forward(self, img):\n        x = img.view(-1, 28*28)\n        x = self.relu(self.linear1(x))\n        x = self.relu(self.linear2(x))\n        x = self.linear3(x)\n        return x\n\nnet = RichBoyNet().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T02:34:45.611613Z","iopub.execute_input":"2024-12-31T02:34:45.611986Z","iopub.status.idle":"2024-12-31T02:34:45.838960Z","shell.execute_reply.started":"2024-12-31T02:34:45.611957Z","shell.execute_reply":"2024-12-31T02:34:45.838274Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"- Train the network only for 1 epoch to simulate a complete general pre-training on the data","metadata":{}},{"cell_type":"code","source":"def train(train_loader, net, epochs=5, total_iterations_limit=None):\n    cross_el = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n\n    total_iterations = 0\n\n    for epoch in range(epochs):\n        net.train()\n\n        loss_sum = 0\n        num_iterations = 0\n\n        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n        if total_iterations_limit is not None:\n            data_iterator.total = total_iterations_limit\n        for data in data_iterator:\n            num_iterations += 1\n            total_iterations += 1\n            x, y = data\n            x = x.to(device)\n            y = y.to(device)\n            optimizer.zero_grad()\n            output = net(x.view(-1, 28*28))\n            loss = cross_el(output, y)\n            loss_sum += loss.item()\n            avg_loss = loss_sum / num_iterations\n            data_iterator.set_postfix(loss=avg_loss)\n            loss.backward()\n            optimizer.step()\n\n            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n                return\n\ntrain(train_loader, net, epochs=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T02:34:49.334050Z","iopub.execute_input":"2024-12-31T02:34:49.334392Z","iopub.status.idle":"2024-12-31T02:35:15.727036Z","shell.execute_reply.started":"2024-12-31T02:34:49.334362Z","shell.execute_reply":"2024-12-31T02:35:15.726280Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 6000/6000 [00:26<00:00, 227.42it/s, loss=0.236]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"- Keep a copy of the original weights (cloning them) so later we can prove that a fine-tuning with LoRA doesn't alter the original weights","metadata":{}},{"cell_type":"code","source":"original_weights = {}\nfor name, param in net.named_parameters():\n    original_weights[name] = param.clone().detach()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T02:35:23.012191Z","iopub.execute_input":"2024-12-31T02:35:23.012579Z","iopub.status.idle":"2024-12-31T02:35:23.017353Z","shell.execute_reply.started":"2024-12-31T02:35:23.012544Z","shell.execute_reply":"2024-12-31T02:35:23.016675Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"- The the performance of the pretrained network. As we can see, the network performs poorly on the digit 9. Let's fine-tune it on the digit 9","metadata":{}},{"cell_type":"code","source":"def test():\n    correct = 0\n    total = 0\n\n    wrong_counts = [0 for i in range(10)]\n\n    with torch.no_grad():\n        for data in tqdm(test_loader, desc='Testing'):\n            x, y = data\n            x = x.to(device)\n            y = y.to(device)\n            output = net(x.view(-1, 784))\n            for idx, i in enumerate(output):\n                if torch.argmax(i) == y[idx]:\n                    correct +=1\n                else:\n                    wrong_counts[y[idx]] +=1\n                total +=1\n    print(f'Accuracy: {round(correct/total, 3)}')\n    for i in range(len(wrong_counts)):\n        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n\ntest()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T02:35:25.916395Z","iopub.execute_input":"2024-12-31T02:35:25.916748Z","iopub.status.idle":"2024-12-31T02:35:28.703026Z","shell.execute_reply.started":"2024-12-31T02:35:25.916722Z","shell.execute_reply":"2024-12-31T02:35:28.702196Z"}},"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 1000/1000 [00:02<00:00, 360.17it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.966\nwrong counts for the digit 0: 21\nwrong counts for the digit 1: 3\nwrong counts for the digit 2: 24\nwrong counts for the digit 3: 40\nwrong counts for the digit 4: 19\nwrong counts for the digit 5: 39\nwrong counts for the digit 6: 39\nwrong counts for the digit 7: 49\nwrong counts for the digit 8: 20\nwrong counts for the digit 9: 83\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"- Let's visualize how many parameters are in the original network, before introducing the LoRA matrices.","metadata":{}},{"cell_type":"code","source":"# Print the size of the weights matrices of the network\n# Save the count of the total number of parameters\ntotal_parameters_original = 0\nfor index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\nprint(f'Total number of parameters: {total_parameters_original:,}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T02:35:31.057088Z","iopub.execute_input":"2024-12-31T02:35:31.057380Z","iopub.status.idle":"2024-12-31T02:35:31.062725Z","shell.execute_reply.started":"2024-12-31T02:35:31.057357Z","shell.execute_reply":"2024-12-31T02:35:31.062026Z"}},"outputs":[{"name":"stdout","text":"Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\nLayer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\nLayer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\nTotal number of parameters: 2,807,010\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"- Define the LoRA parameterization as described in the paper. The full detail on how PyTorch parameterizations work is here: https://pytorch.org/tutorials/intermediate/parametrizations.html","metadata":{}},{"cell_type":"code","source":"class LoRAParametrization(nn.Module):\n    def __init__(self, features_in, features_out, rank=1, alpha=1, device='cpu'):\n        super().__init__()\n        # Section 4.1 of the paper: \n        #   We use a random Gaussian initialization for A and zero for B, so ∆W = BA is zero at the beginning of training\n        self.lora_A = nn.Parameter(torch.zeros((rank,features_out)).to(device))\n        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n        nn.init.normal_(self.lora_A, mean=0, std=1)\n        \n        # Section 4.1 of the paper: \n        #   We then scale ∆Wx by α/r , where α is a constant in r. \n        #   When optimizing with Adam, tuning α is roughly the same as tuning the learning rate if we scale the initialization appropriately. \n        #   As a result, we simply set α to the first r we try and do not tune it. \n        #   This scaling helps to reduce the need to retune hyperparameters when we vary r.\n        self.scale = alpha / rank\n        self.enabled = True\n\n    def forward(self, original_weights):\n        if self.enabled:\n            # Return W + (B*A)*scale\n            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n        else:\n            return original_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T02:35:32.578873Z","iopub.execute_input":"2024-12-31T02:35:32.579156Z","iopub.status.idle":"2024-12-31T02:35:32.584622Z","shell.execute_reply.started":"2024-12-31T02:35:32.579134Z","shell.execute_reply":"2024-12-31T02:35:32.583848Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":" - Add the parameterization to our network.","metadata":{}},{"cell_type":"code","source":"import torch.nn.utils.parametrize as parametrize\n\ndef linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n    # Only add the parameterization to the weight matrix, ignore the Bias\n\n    # From section 4.2 of the paper:\n    #   We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency.\n    #   [...]\n    #   We leave the empirical investigation of [...], and biases to a future work.\n    \n    features_in, features_out = layer.weight.shape\n    return LoRAParametrization(\n        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n    )\n\nparametrize.register_parametrization(\n    net.linear1, \"weight\", linear_layer_parameterization(net.linear1, device)\n)\nparametrize.register_parametrization(\n    net.linear2, \"weight\", linear_layer_parameterization(net.linear2, device)\n)\nparametrize.register_parametrization(\n    net.linear3, \"weight\", linear_layer_parameterization(net.linear3, device)\n)\n\n\ndef enable_disable_lora(enabled=True):\n    for layer in [net.linear1, net.linear2, net.linear3]:\n        layer.parametrizations[\"weight\"][0].enabled = enabled","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T02:35:35.672685Z","iopub.execute_input":"2024-12-31T02:35:35.673012Z","iopub.status.idle":"2024-12-31T02:35:35.745319Z","shell.execute_reply.started":"2024-12-31T02:35:35.672981Z","shell.execute_reply":"2024-12-31T02:35:35.744623Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"- Display the number of parameters added by LoRA.","metadata":{}},{"cell_type":"code","source":"total_parameters_lora = 0\ntotal_parameters_non_lora = 0\nfor index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_A.nelement() + layer.parametrizations[\"weight\"][0].lora_B.nelement()\n    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n    print(\n        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}'\n    )\n# The non-LoRA parameters count must match the original network\nassert total_parameters_non_lora == total_parameters_original\nprint(f'Total number of parameters (original): {total_parameters_non_lora:,}')\nprint(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\nprint(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\nparameters_incremment = (total_parameters_lora / total_parameters_non_lora) * 100\nprint(f'Parameters incremment: {parameters_incremment:.3f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T02:35:39.259075Z","iopub.execute_input":"2024-12-31T02:35:39.259386Z","iopub.status.idle":"2024-12-31T02:35:39.267909Z","shell.execute_reply.started":"2024-12-31T02:35:39.259360Z","shell.execute_reply":"2024-12-31T02:35:39.267068Z"}},"outputs":[{"name":"stdout","text":"Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000]) + Lora_A: torch.Size([1, 784]) + Lora_B: torch.Size([1000, 1])\nLayer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000]) + Lora_A: torch.Size([1, 1000]) + Lora_B: torch.Size([2000, 1])\nLayer 3: W: torch.Size([10, 2000]) + B: torch.Size([10]) + Lora_A: torch.Size([1, 2000]) + Lora_B: torch.Size([10, 1])\nTotal number of parameters (original): 2,807,010\nTotal number of parameters (original + LoRA): 2,813,804\nParameters introduced by LoRA: 6,794\nParameters incremment: 0.242%\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"- Freeze all the parameters of the original network and only fine tuning the ones introduced by LoRA. Then fine-tune the model on the digit 9 and only for 100 batches.","metadata":{}},{"cell_type":"code","source":"# Freeze the non-Lora parameters\nfor name, param in net.named_parameters():\n    if 'lora' not in name:\n        print(f'Freezing non-LoRA parameter {name}')\n        param.requires_grad = False\n\n# Load the MNIST dataset again, by keeping only the digit 9\nmnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\nexclude_indices = mnist_trainset.targets == 9\nmnist_trainset.data = mnist_trainset.data[exclude_indices]\nmnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n# Create a dataloader for the training\ntrain_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n\n# Train the network with LoRA only on the digit 9 and only for 100 batches (hoping that it would improve the performance on the digit 9)\ntrain(train_loader, net, epochs=1, total_iterations_limit=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T02:35:41.203751Z","iopub.execute_input":"2024-12-31T02:35:41.204036Z","iopub.status.idle":"2024-12-31T02:35:41.819248Z","shell.execute_reply.started":"2024-12-31T02:35:41.204015Z","shell.execute_reply":"2024-12-31T02:35:41.818470Z"}},"outputs":[{"name":"stdout","text":"Freezing non-LoRA parameter linear1.bias\nFreezing non-LoRA parameter linear1.parametrizations.weight.original\nFreezing non-LoRA parameter linear2.bias\nFreezing non-LoRA parameter linear2.parametrizations.weight.original\nFreezing non-LoRA parameter linear3.bias\nFreezing non-LoRA parameter linear3.parametrizations.weight.original\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  99%|█████████▉| 99/100 [00:00<00:00, 194.39it/s, loss=0.086] \n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"- Verify that the fine-tuning didn't alter the original weights, but only the ones introduced by LoRA.","metadata":{}},{"cell_type":"code","source":"# Check that the frozen parameters are still unchanged by the finetuning\nassert torch.all(net.linear1.parametrizations.weight.original == original_weights['linear1.weight'])\nassert torch.all(net.linear2.parametrizations.weight.original == original_weights['linear2.weight'])\nassert torch.all(net.linear3.parametrizations.weight.original == original_weights['linear3.weight'])\n\nenable_disable_lora(enabled=True)\n# The new linear1.weight is obtained by the \"forward\" function of our LoRA parametrization\n# The original weights have been moved to net.linear1.parametrizations.weight.original\n# More info here: https://pytorch.org/tutorials/intermediate/parametrizations.html#inspecting-a-parametrized-module\nassert torch.equal(net.linear1.weight, net.linear1.parametrizations.weight.original + (net.linear1.parametrizations.weight[0].lora_B @ net.linear1.parametrizations.weight[0].lora_A) * net.linear1.parametrizations.weight[0].scale)\n\nenable_disable_lora(enabled=False)\n# If we disable LoRA, the linear1.weight is the original one\nassert torch.equal(net.linear1.weight, original_weights['linear1.weight'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T02:35:44.184628Z","iopub.execute_input":"2024-12-31T02:35:44.184940Z","iopub.status.idle":"2024-12-31T02:35:44.217308Z","shell.execute_reply.started":"2024-12-31T02:35:44.184912Z","shell.execute_reply":"2024-12-31T02:35:44.216501Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":" - Test the network with LoRA enabled (the digit 9 should be classified better)","metadata":{"execution":{"iopub.status.busy":"2024-12-31T02:28:19.855543Z","iopub.execute_input":"2024-12-31T02:28:19.855819Z","iopub.status.idle":"2024-12-31T02:28:19.860970Z","shell.execute_reply.started":"2024-12-31T02:28:19.855798Z","shell.execute_reply":"2024-12-31T02:28:19.859863Z"}}},{"cell_type":"code","source":"# Test with LoRA enabled\nenable_disable_lora(enabled=True)\ntest()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T02:35:45.571933Z","iopub.execute_input":"2024-12-31T02:35:45.572212Z","iopub.status.idle":"2024-12-31T02:35:48.684116Z","shell.execute_reply.started":"2024-12-31T02:35:45.572191Z","shell.execute_reply":"2024-12-31T02:35:48.683292Z"}},"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 1000/1000 [00:03<00:00, 322.26it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.934\nwrong counts for the digit 0: 26\nwrong counts for the digit 1: 8\nwrong counts for the digit 2: 41\nwrong counts for the digit 3: 81\nwrong counts for the digit 4: 135\nwrong counts for the digit 5: 95\nwrong counts for the digit 6: 52\nwrong counts for the digit 7: 78\nwrong counts for the digit 8: 128\nwrong counts for the digit 9: 18\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"- Test the network with LoRA disabled (the accuracy and errors counts must be the same as the original network)","metadata":{}},{"cell_type":"code","source":"# Test with LoRA disabled\nenable_disable_lora(enabled=False)\ntest()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T02:35:50.327298Z","iopub.execute_input":"2024-12-31T02:35:50.327697Z","iopub.status.idle":"2024-12-31T02:35:53.099643Z","shell.execute_reply.started":"2024-12-31T02:35:50.327658Z","shell.execute_reply":"2024-12-31T02:35:53.098876Z"}},"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 1000/1000 [00:02<00:00, 361.81it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.966\nwrong counts for the digit 0: 21\nwrong counts for the digit 1: 3\nwrong counts for the digit 2: 24\nwrong counts for the digit 3: 40\nwrong counts for the digit 4: 19\nwrong counts for the digit 5: 39\nwrong counts for the digit 6: 39\nwrong counts for the digit 7: 49\nwrong counts for the digit 8: 20\nwrong counts for the digit 9: 83\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}